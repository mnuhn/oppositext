# OpposiText - Transformer Model to generate "opposite text".

OpposiText is a transformer model based on the T5 architecture. The
model is bootstrapped from the T5-small baseline.

The model generates sentences with an inverted / negated / opposite meaning of
it's input sentence. Overall, the project focuses on sentences of up to 12
words.

**Examples:**

Input | Output
---|---
Good. | Bad.
Today is a great day. | Today is a bad day.
Generate the oppisite text. | Allow the same text.
Eat the rich. | Eat the poor.
Use SFT and PPO for training. | Don't use SFT and PPO for training.
The investigation took place in 2024 | The investigation didn't take place in 2024.
I am happy.	| I am sad.
The weather is sunny. | The weather is rainy.
He is strong. | He is weak.

Overall, the model should avoid "hallucinations" and driftig away too far from
the input sentence. To achieve this, the following techniques are used.

Bootstrapping the model:
1. Supervised fine-tuning
2. Training data augmentation

Avoiding "hallucinations" and "topical drift":
3. Reward modeling
4. RLHF using PPO

I evaluate the models using ELO ratings to show how the techniques improve the
performance of the model.

# Model
T5-Small [paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf).

> Small. We consider a smaller model, which scales the baseline down by using
> dmodel = 512, dff = 2,048, 8-headed attention, and only 6 layers each in the
> encoder and decoder. This variant has about 60 million parameters.


# Definition of the Task

For an input sentence, return a sentence with the opposite meaning. For a
simple sentence like "X is Y" it should be "X is not Y" or "X is Z" where Z is
an antonym.

For more complex sentences like "X is Y and Z" it should be "X is not Y and not
Z".

For questions, asking a different question is good enough.

X if Y -> Not X if Not Y.

Do X to achieve Y.
Do not do X to achieve not Y.


# Base model: T5-simple
The base model used for this task is the T5 simple. [ADD DESCRIPTION]

# Supervised Fine-Tuning

I do several iterations of getting training data.

## Bootstrapping: A list of antonyms.
I collect a list of 400 antonyms. I use the fact that "A = NOT(B)" also implies
that "B = NOT(A)" so we can double the training data.

## Simple sentences.

## Mining opposites.
I implement sft_viewer.py. I run the model against a corpus of prompts. I then
get the outputs. I pre-filter the outputs and then manually select what's OK.
This is added to the SFT examples.

# RLHF

## Reward Modeling

### Rule-Based

Part 1: $\alpha$:
* Don't produce identical outputs.
* The length should be similar
* Punctuation should be similar

Part 2: $\beta$:
* Not all the words should change (jaccard index over words)
* No excessive reperition of words

This overall rule reward is then $\alpha \cdot \beta$.

### Preference Data
I implement ppo-viewer. Present 2 alternatives, and I pick.

Overall, the reward model didn't really learn that e.g. the identity is to be
avoided.

I did two things:
1) I generated a list of preference pairs from the decoding on a prompt corpus.
Base is the identity (if it was generated by the model) and Winner is the best
output as proposed by the model.

2) I mix the rule-based reward model with the learned reward model.

